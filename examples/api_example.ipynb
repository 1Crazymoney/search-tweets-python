{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Twitter Search APIs Python Wrapper\n",
    "\n",
    "Working with the API within a Python program is straightforward both for Premium and Enterprise clients.\n",
    "\n",
    "Our group's python [tweet parser library](https://github.com/twitterdev/tweet_parser) is a requirement.\n",
    "\n",
    "\n",
    "Prior to starting your program, an easy way to define your secrets will be setting an environment variable. If you are an enterprise client, your authentication will be a (username, password) pair. If you are a premium client, you'll need to get a bearer token that will be passed with each call for authentication.\n",
    "\n",
    "Your credentials should be put into a YAML file that looks like this:\n",
    "\n",
    "\n",
    "```.yaml\n",
    "\n",
    "search_tweets_api:\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  account: <ACCOUNT_NAME>\n",
    "  username: <USERNAME>\n",
    "  password: <PW>\n",
    "  bearer_token: <TOKEN>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "And filling in the keys that are appropriate for your account type. Premium users should only have the `endpoint` and `bearer_token`; Enterprise customers should have `account`, `username`, `endpoint`, and `password`.\n",
    "\n",
    "\n",
    "Our credential reader will default this file being in `\"~/.twitter_keys.yaml\"`, but you can pass the relevant location as needed. You can also specify a different key in the yaml file, which can be useful if you have different endpoints, e.g., `dev`, `test`, `prod`, etc. The file might look like this:\n",
    "\n",
    "```.yaml\n",
    "\n",
    "search_tweets_dev:\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  bearer_token: <TOKEN>\n",
    "  \n",
    "search_tweets_prod:\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  bearer_token: <TOKEN>\n",
    "  \n",
    "```\n",
    "\n",
    "The following cell demonstrates the basic setup that will be referenced throughout your program's session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enterprise setup\n",
    "\n",
    "If you are an enterprise customer, you'll need to authenticate with a basic username/password method. You can specify that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enterprise_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n",
    "                                          account_type=\"enterprise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premium Setup\n",
    "\n",
    "Premium customers will use a bearer token for authentication. Use the following cell for setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premium_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_premium\",\n",
    "                                       account_type=\"premium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function that formats search API rules into valid json queries called `gen_rule_payload`. It has sensible defaults, such as pulling more tweets per call than the default 100 (but note that a sandbox environment can only have a max of 100 here, so if you get errors, please check this) not including dates, and defaulting to hourly counts when using the counts api. Discussing the finer points of generating search rules is out of scope for these examples; I encourage you to see the docs to learn the nuances within, but for now let's see what a rule looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"beyonce\",\"maxResults\":100}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"beyonce\", results_per_call=100) # testing with a sandbox account\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule will match tweets that have the text `beyonce` in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, there are two ways to interact with the API. There is a quick method to collect smaller amounts of tweets to memory that requires less thought and knowledge, and interaction with the `ResultStream` object which will be introduced later.\n",
    "\n",
    "\n",
    "## Fast Way\n",
    "\n",
    "We'll use the `search_args` variable to power the configuration point for the API. The object also takes a valid PowerTrack rule and has options to cutoff search when hitting limits on both number of tweets and API calls.\n",
    "\n",
    "We'll be using the `collect_results` function, which has three parameters.\n",
    "\n",
    "- rule: a valid PowerTrack rule, referenced earlier\n",
    "- max_results: as the API handles pagination, it will stop collecting when we get to this number\n",
    "- result_stream_args: configuration args that we've already specified.\n",
    "\n",
    "\n",
    "For the remaining examples, please change the args to either premium or enterprise depending on your usage.\n",
    "\n",
    "Let's see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from searchtweets import collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = collect_results(rule,\n",
    "                         max_results=100,\n",
    "                         result_stream_args=enterprise_search_args) # change this if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, tweet payloads are lazily parsed into a `Tweet` object. An overwhelming number of tweet attributes are made available directly, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was okay for Beyonce to stay when Jay Z cheated but not okay for Cardi to stay with Offset. You people and your double standards man.\n",
      "\n",
      "@Captivate 🔥🔥 Black Kings &amp; Queens🔥🔥 \n",
      "https://t.co/RvpJNpKEA1\n",
      "#tidal #Spotify #blackish #BlackTwitter #BlackExcellence #RevoltNow @BadBoyEnt @RocNation @djkhaled @S_C_ @DJInfamousATL @djenvy @Diddy @Beyonce #bmore #baltimorecity #share  IG; kingdavid_2022\n",
      "\n",
      "*Beyoncé comes on* Friends: please don't do it i swear it's so embarras-- \n",
      "\n",
      "Me: https://t.co/tqup7M67jI\n",
      "\n",
      "Somebody just said Beyoncé gone release the twins on tidal. https://t.co/4kmk8w9pFf\n",
      "\n",
      "los gringos tienen como 200k d rts acá tenemos 2k y nos sentimos beyoncé\n",
      "\n",
      "@BarbaraLafranc2 🔥🔥 Black Kings &amp; Queens🔥🔥 \n",
      "https://t.co/RvpJNpKEA1\n",
      "#tidal #Spotify #blackish #BlackTwitter #BlackExcellence #RevoltNow @BadBoyEnt @RocNation @djkhaled @S_C_ @DJInfamousATL @djenvy @Diddy @Beyonce #bmore #baltimorecity #share  IG; kingdavid_2022\n",
      "\n",
      "The president of the United States is busy calling this country a shithole, meanwhile Beyoncé’s charity is entering its EIGTH year of supporting their charities after a devastating earthquake killed thousands.  https://t.co/9qVtPQKp8W\n",
      "8 years ago today an earthquake hit Haiti that devastated families. We responded and launched #BEYGOODHAITI to help revitalize Saint Damien Pediatric Hospital. We remain in partnership with them and encourage you to also support: https://t.co/Sb1AS8rA4g https://t.co/iMuk00Zllv\n",
      "\n",
      "So after a few days I finally figured out witch song has the best Intro and everyone agreed with me 😂 It has to be Beyoncé ... https://t.co/cc5rcJD1YJ\n",
      "\n",
      "Jay Z and Beyoncé don't even follow each other. That's a real relationship goal bitch mind ya business.\n",
      "\n",
      "Hold Up by Beyoncé \n",
      "#BadLiar #BestMusicVideo #iHeartAwards https://t.co/fTgMBccdc1\n",
      "78. If Selena had to reenact and lip sync to this Music Video, which one you want it to be, 'Hold Up' by Beyonce or 'Side To Side' by Ariana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.all_text, end='\\n\\n') for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-12 21:05:39\n",
      "2018-01-12 21:05:39\n",
      "2018-01-12 21:05:36\n",
      "2018-01-12 21:05:34\n",
      "2018-01-12 21:05:34\n",
      "2018-01-12 21:05:33\n",
      "2018-01-12 21:05:32\n",
      "2018-01-12 21:05:31\n",
      "2018-01-12 21:05:31\n",
      "2018-01-12 21:05:30\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.created_at_datetime) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for Android\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.generator.get(\"name\")) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, we have some tweets. For interactive environments and other cases where you don't care about collecting your data in a single load or don't need to operate on the stream of tweets or counts directly, I recommend using this convenience function.\n",
    "\n",
    "\n",
    "## Working with the ResultStream\n",
    "\n",
    "The ResultStream object will be powered by the `search_args`, and takes the rules and other configuration parameters, including a hard stop on number of pages to limit your API call usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultStream: \n",
      "\t{\n",
      "    \"username\":null,\n",
      "    \"endpoint\":\"https:\\/\\/api.twitter.com\\/1.1\\/tweets\\/search\\/30day\\/dev.json\",\n",
      "    \"rule_payload\":{\n",
      "        \"query\":\"beyonce\",\n",
      "        \"maxResults\":100\n",
      "    },\n",
      "    \"tweetify\":true,\n",
      "    \"max_results\":500\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=500,\n",
    "                  max_pages=1,\n",
    "                  **premium_search_args)\n",
    "\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function, `.stream`, that seamlessly handles requests and pagination for a given query. It returns a generator, and to grab our 500 tweets that mention `beyonce` we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = list(rs.stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are lazily parsed using our Tweet Parser, so tweet data is very easily extractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me when Beyoncé disappears for days. https://t.co/jPBt94K9xR\n",
      "Why is it okay for\n",
      "Beyoncé to make $50\n",
      "million and not okay\n",
      "for a CEO who has\n",
      "3000 employees and\n",
      "$100 million in profit to\n",
      "make $5 million?\n",
      "Just saw some dude say Tomi Lahren look better than Beyonce\n",
      "\n",
      "...boy https://t.co/9YsVVMcEqy\n",
      "@writemombritt @GAPeachMeg @skb_sara @PaulLee85 @TheSlimSupreme @MistaBRONCO @TheBeard1611 @Redheaded_Jenn @Keque_Mage @Flewbys @W_C_Patriot Jay Z watches Beyoncé kissing Barack Obama\n",
      "A partir du moment ou un homme qui était dans une télé-réalité et sur un ring de catch se retrouve a la tête de la 1ere puissance mondiale j'exclu plus rien dans ma vie, donc la j'ai comme objectif de baiser Beyoncé\n",
      "23) ANYTHING FOR YOU BEYONCE\n",
      "https://t.co/MoZNaAoT0i\n",
      "Cardi B ties Beyonce’s Billboard Hot R&amp;B/Hip-Hop songs record https://t.co/wd2EIBC0zM https://t.co/S1Ul8wqO41\n",
      "BEYONCÉ still holds the record for #1s in the most countries on iTunes when it topped 117 charts in 2013 simultaneously. https://t.co/XTcfncnWzj\n",
      "I love Beyoncé but she is a beautiful demon Michelle looks like she’s in an abusive relationship https://t.co/HvGngt4iCk\n",
      "future sings with way more passion that beyoncé if we keeping it a buck\n"
     ]
    }
   ],
   "source": [
    "# using unidecode to prevent emoji/accents printing \n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts Endpoint\n",
    "\n",
    "We can also use the Search API Counts endpoint to get counts of tweets that match our rule. Each request will return up to *30* results, and each count request can be done on a minutely, hourly, or daily basis. The underlying `ResultStream` object will handle converting your endpoint to the count endpoint, and you have to specify the `count_bucket` argument when making a rule to use it.\n",
    "\n",
    "The process is very similar to grabbing tweets, but has some minor differences.\n",
    "\n",
    "\n",
    "_Caveat - premium sandbox environments do NOT have access to the Search API counts endpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_rule = gen_rule_payload(\"beyonce\", count_bucket=\"day\")\n",
    "\n",
    "counts = collect_results(count_rule, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are pretty straightforward and can be rapidly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': 41513, 'timePeriod': '201801120000'},\n",
       " {'count': 42012, 'timePeriod': '201801110000'},\n",
       " {'count': 47334, 'timePeriod': '201801100000'},\n",
       " {'count': 66070, 'timePeriod': '201801090000'},\n",
       " {'count': 96729, 'timePeriod': '201801080000'},\n",
       " {'count': 162544, 'timePeriod': '201801070000'},\n",
       " {'count': 105965, 'timePeriod': '201801060000'},\n",
       " {'count': 93191, 'timePeriod': '201801050000'},\n",
       " {'count': 110430, 'timePeriod': '201801040000'},\n",
       " {'count': 127657, 'timePeriod': '201801030000'},\n",
       " {'count': 132053, 'timePeriod': '201801020000'},\n",
       " {'count': 176279, 'timePeriod': '201801010000'},\n",
       " {'count': 57287, 'timePeriod': '201712310000'},\n",
       " {'count': 72341, 'timePeriod': '201712300000'},\n",
       " {'count': 72151, 'timePeriod': '201712290000'},\n",
       " {'count': 76440, 'timePeriod': '201712280000'},\n",
       " {'count': 61644, 'timePeriod': '201712270000'},\n",
       " {'count': 55203, 'timePeriod': '201712260000'},\n",
       " {'count': 59181, 'timePeriod': '201712250000'},\n",
       " {'count': 106356, 'timePeriod': '201712240000'},\n",
       " {'count': 115224, 'timePeriod': '201712230000'},\n",
       " {'count': 73473, 'timePeriod': '201712220000'},\n",
       " {'count': 89280, 'timePeriod': '201712210000'},\n",
       " {'count': 192571, 'timePeriod': '201712200000'},\n",
       " {'count': 85625, 'timePeriod': '201712190000'},\n",
       " {'count': 57924, 'timePeriod': '201712180000'},\n",
       " {'count': 70558, 'timePeriod': '201712170000'},\n",
       " {'count': 41087, 'timePeriod': '201712160000'},\n",
       " {'count': 62799, 'timePeriod': '201712150000'},\n",
       " {'count': 55363, 'timePeriod': '201712140000'},\n",
       " {'count': 98255, 'timePeriod': '201712130000'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dated searches / Full Archive Search\n",
    "\n",
    "\n",
    "Let's make a new rule and pass it dates this time.\n",
    "\n",
    "`gen_rule_payload` takes dates of the forms `YYYY-mm-DD` and `YYYYmmDD`.\n",
    "\n",
    "\n",
    "**Note that this will only work with the full archive search option**, which is available to my account only via the enterprise options. Full archive search will likely require a different endpoint or access method; please see your developer console for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"from:jack\",\"maxResults\":500,\"toDate\":\"201710300000\",\"fromDate\":\"201709010000\"}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"from:jack\", from_date=\"2017-09-01\", to_date=\"2017-10-30\", results_per_call=500)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More clarity on our private information policy and enforcement. Working to build as much direct context into the product too https://t.co/IrwBexPrBA\n",
      "To provide more clarity on our private information policy, we’ve added specific examples of what is/is not a violation and insight into what we need to remove this type of content from the service. https://t.co/NGx5hh2tTQ\n",
      "Launching violent groups and hateful images/symbols policy on November 22nd https://t.co/NaWuBPxyO5\n",
      "We will now launch our policies on violent groups and hateful imagery and hate symbols on Nov 22. During the development process, we received valuable feedback that we’re implementing before these are published and enforced. See more on our policy development process here 👇 https://t.co/wx3EeH39BI\n",
      "@WillStick @lizkelley Happy birthday Liz!\n",
      "Off-boarding advertising from all accounts owned by Russia Today (RT) and Sputnik.\n",
      "\n",
      "We’re donating all projected earnings ($1.9mm) to support external research into the use of Twitter in elections, including use of malicious automation and misinformation. https://t.co/zIxfqqXCZr\n",
      "@TMFJMo @anthonynoto Thank you\n",
      "@gasca @stratechery @Lefsetz letter\n",
      "@gasca @stratechery Bridgewater’s Daily Observations\n",
      "Yup!!!! ❤️❤️❤️❤️ #davechappelle https://t.co/ybSGNrQpYF\n",
      "@ndimichino Sometimes\n",
      "Setting up at @CampFlogGnaw https://t.co/nVq8QjkKsf\n"
     ]
    }
   ],
   "source": [
    "# usiing unidecode only to \n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"from:jack\",\"toDate\":\"201710300000\",\"fromDate\":\"201709200000\",\"bucket\":\"day\"}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"from:jack\",\n",
    "                        from_date=\"2017-09-20\",\n",
    "                        to_date=\"2017-10-30\",\n",
    "                        count_bucket=\"day\",\n",
    "                        results_per_call=500)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timePeriod': '201710290000', 'count': 0}\n",
      "{'timePeriod': '201710280000', 'count': 0}\n",
      "{'timePeriod': '201710270000', 'count': 3}\n",
      "{'timePeriod': '201710260000', 'count': 6}\n",
      "{'timePeriod': '201710250000', 'count': 4}\n",
      "{'timePeriod': '201710240000', 'count': 4}\n",
      "{'timePeriod': '201710230000', 'count': 0}\n",
      "{'timePeriod': '201710220000', 'count': 0}\n",
      "{'timePeriod': '201710210000', 'count': 3}\n",
      "{'timePeriod': '201710200000', 'count': 2}\n",
      "{'timePeriod': '201710190000', 'count': 1}\n",
      "{'timePeriod': '201710180000', 'count': 6}\n",
      "{'timePeriod': '201710170000', 'count': 2}\n",
      "{'timePeriod': '201710160000', 'count': 2}\n",
      "{'timePeriod': '201710150000', 'count': 1}\n",
      "{'timePeriod': '201710140000', 'count': 64}\n",
      "{'timePeriod': '201710130000', 'count': 3}\n",
      "{'timePeriod': '201710120000', 'count': 4}\n",
      "{'timePeriod': '201710110000', 'count': 8}\n",
      "{'timePeriod': '201710100000', 'count': 4}\n",
      "{'timePeriod': '201710090000', 'count': 1}\n",
      "{'timePeriod': '201710080000', 'count': 0}\n",
      "{'timePeriod': '201710070000', 'count': 0}\n",
      "{'timePeriod': '201710060000', 'count': 1}\n",
      "{'timePeriod': '201710050000', 'count': 3}\n",
      "{'timePeriod': '201710040000', 'count': 5}\n",
      "{'timePeriod': '201710030000', 'count': 8}\n",
      "{'timePeriod': '201710020000', 'count': 5}\n",
      "{'timePeriod': '201710010000', 'count': 0}\n",
      "{'timePeriod': '201709300000', 'count': 0}\n",
      "{'timePeriod': '201709290000', 'count': 0}\n",
      "{'timePeriod': '201709280000', 'count': 9}\n",
      "{'timePeriod': '201709270000', 'count': 41}\n",
      "{'timePeriod': '201709260000', 'count': 13}\n",
      "{'timePeriod': '201709250000', 'count': 6}\n",
      "{'timePeriod': '201709240000', 'count': 7}\n",
      "{'timePeriod': '201709230000', 'count': 3}\n",
      "{'timePeriod': '201709220000', 'count': 0}\n",
      "{'timePeriod': '201709210000', 'count': 1}\n",
      "{'timePeriod': '201709200000', 'count': 7}\n"
     ]
    }
   ],
   "source": [
    "[print(c) for c in counts];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
