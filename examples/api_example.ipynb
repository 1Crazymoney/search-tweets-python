{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Twitter Search APIs Python Wrapper\n",
    "\n",
    "Working with the API within a Python program is straightforward both for Premium and Enterprise clients.\n",
    "\n",
    "Our group's python [tweet parser library](https://github.com/twitterdev/tweet_parser) is a requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credential Handling\n",
    "\n",
    "The premium and enterprise Search APIs use different credentials and we attempt to provide methods of seamless handling for all customers. \n",
    "We support YAML-file based methods and environment variables for access.\n",
    "\n",
    "A YAML credential file should look like this:\n",
    "\n",
    "```.yaml\n",
    "\n",
    "<key>:\n",
    "  account_type: <OPTIONAL PREMIUM_OR_ENTERPRISE>\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  username: <USERNAME>\n",
    "  password: <PW>\n",
    "  bearer_token: <TOKEN>\n",
    "```\n",
    "\n",
    "Premium clients will require the `bearer_token` and `endpoint` fields; Enterprise clients require `username`, `password`, and `endpoint`. If you do not specify the `account_type`, we attempt to discern the account type and declare a warning about this behavior. The `load_credentials` function also allows `account_type` to be set.\n",
    "\n",
    "Our credential reader will default this file being located at `\"~/.twitter_keys.yaml\"`, but you can pass the relevant location as needed.\n",
    "You can also specify a different key in the yaml file, which can be useful if you have different endpoints, e.g., `dev`, `test`, `prod`, etc. The file might look like this:\n",
    "\n",
    "\n",
    "```.yaml\n",
    "\n",
    "search_tweets_dev:\n",
    "  account_type: premium\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  bearer_token: <TOKEN>\n",
    "  \n",
    "search_tweets_prod:\n",
    "  account_type: premium\n",
    "  endpoint: <FULL_URL_OF_ENDPOINT>\n",
    "  bearer_token: <TOKEN>\n",
    "  \n",
    "```\n",
    "\n",
    "If you want or need to pass credentials via environment variables, you can set the appropriate variables of the following: \n",
    "\n",
    "```\n",
    "export SEARCHTWEETS_ENDPOINT=\n",
    "export SEARCHTWEETS_USERNAME=\n",
    "export SEARCHTWEETS_PASSWORD=\n",
    "export SEARCHTWEETS_BEARER_TOKEN=\n",
    "export SEARCHTWEETS_ACCOUNT_TYPE=\n",
    "```\n",
    "\n",
    "The `load_credentials` function will attempt to find these variables if it cannot load fields from the yaml file, and it will **overwrite any found credentials from the YAML file** if they have been parsed. This behavior can be changed by setting the `load_credentials` parameter `env_overwrite` to `False`.\n",
    "\n",
    "\n",
    "The following cells demonstrates credential handling, both in the command line app and Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': '<MY_ENDPOINT>',\n",
       " 'password': '<MY_PASSWORD>',\n",
       " 'username': '<MY_USERNAME>'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_credentials(filename=\"./search_tweets_creds_example.yaml\",\n",
    "                 yaml_key=\"search_tweets_ent_example\",\n",
    "                 env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bearer_token': '<A_VERY_LONG_MAGIC_STRING>',\n",
       " 'endpoint': 'https://api.twitter.com/1.1/tweets/search/30day/dev.json'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_credentials(filename=\"./search_tweets_creds_example.yaml\",\n",
    "                 yaml_key=\"search_tweets_premium_example\",\n",
    "                 env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variable Overrides\n",
    "\n",
    "If we set our environment variables, the program will look for them regardless of a YAML file's validity or existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot read file nothing\n",
      "Error parsing YAML file; searching for valid environment variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'https://endpoint',\n",
       " 'password': 'ENV_PW',\n",
       " 'username': 'ENV_USERNAME'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"SEARCHTWEETS_USERNAME\"] = \"ENV_USERNAME\"\n",
    "os.environ[\"SEARCHTWEETS_PASSWORD\"] = \"ENV_PW\"\n",
    "os.environ[\"SEARCHTWEETS_ENDPOINT\"] = \"https://endpoint\"\n",
    "\n",
    "load_credentials(filename=\"nothing\", yaml_key=\"no_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search API usage\n",
    "\n",
    "We'll now load our proper credentials and move on with the example.\n",
    "\n",
    "### Enterprise setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enterprise_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n",
    "                                          yaml_key=\"search_tweets_enterprise\",\n",
    "                                          env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premium Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premium_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_premium\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function that formats search API rules into valid json queries called `gen_rule_payload`. It has sensible defaults, such as pulling more tweets per call than the default 100 (but note that a sandbox environment can only have a max of 100 here, so if you get errors, please check this) not including dates, and defaulting to hourly counts when using the counts api. Discussing the finer points of generating search rules is out of scope for these examples; I encourage you to see the docs to learn the nuances within, but for now let's see what a rule looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"beyonce\",\"maxResults\":100}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"beyonce\", results_per_call=100) # testing with a sandbox account\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule will match tweets that have the text `beyonce` in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, there are two ways to interact with the API. There is a quick method to collect smaller amounts of tweets to memory that requires less thought and knowledge, and interaction with the `ResultStream` object which will be introduced later.\n",
    "\n",
    "\n",
    "## Fast Way\n",
    "\n",
    "We'll use the `search_args` variable to power the configuration point for the API. The object also takes a valid PowerTrack rule and has options to cutoff search when hitting limits on both number of tweets and API calls.\n",
    "\n",
    "We'll be using the `collect_results` function, which has three parameters.\n",
    "\n",
    "- rule: a valid PowerTrack rule, referenced earlier\n",
    "- max_results: as the API handles pagination, it will stop collecting when we get to this number\n",
    "- result_stream_args: configuration args that we've already specified.\n",
    "\n",
    "\n",
    "For the remaining examples, please change the args to either premium or enterprise depending on your usage.\n",
    "\n",
    "Let's see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from searchtweets import collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = collect_results(rule,\n",
    "                         max_results=100,\n",
    "                         result_stream_args=enterprise_search_args) # change this if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, tweet payloads are lazily parsed into a `Tweet` object. An overwhelming number of tweet attributes are made available directly, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Beyonc√© asked me to quit my job and go on tour with her.....I WILL BE A MAMA.\n",
      "\n",
      "Justin Timberlake Super Bowl Setlist:\n",
      "\n",
      "Filthy (obvious promo)\n",
      "Man In The Mirror\n",
      "Holy Grail ft. Jay Z &amp; Beyonc√©\n",
      "Formation \n",
      "End of Time\n",
      "Run The World\n",
      "Crazy In Love \n",
      "Single Ladies\n",
      "‚ÄúPut your hands towards me, I want to feel your energy‚Äù Halo\n",
      "\n",
      "God bless y‚Äôall! https://t.co/5WdnxmLzjE\n",
      "\n",
      "If Beyonc√© called me and asked me to quit my job and go on tour with her but she‚Äôs not paying me...... I WOULD STILL GO. https://t.co/1iIUvDjyVc\n",
      "\n",
      "@edsheeran Beyonc√© is not credited on \"Perfect\" for this week as the solo version of the song has been the dominant version this past week.\n",
      "\n",
      "¬´¬†quand Beyonce a dit ¬´¬†c‚Äôest pas la tromperie qui fait mal, c‚Äôest le genre de personne avec qui il me trompe, que mnt je dois le regarder dans les yeux et savoir qu‚Äôelle avait la moiti√© de ce que j‚Äôai alors qu‚Äôelle n‚Äôest m√™me pas la moiti√© de ce que je suis¬†¬ª j‚Äôai compris\n",
      "\n",
      "If @Beyonce played as Nala in The Lion King (lmao I tried, it‚Äôs hard to sound like her üíÄ) https://t.co/vH5sYIpKVj\n",
      "\n",
      "@PopCrave @edsheeran lol at hip and knee fans aka the vegas dwellers celebrating under this tweet. The duet helped the original to reach #1 for 5 weeks but Beyonc√© never cared about it. Now the solo is doing better but it doesn‚Äôt mean the duet is flopping. What‚Äôs the tea? Beyonc√© &gt; fagda and knee. https://t.co/z3LGSeLRCh\n",
      "\n",
      "Beyonc√© if she became an algebra teacher and married an engineer https://t.co/s2m1qg9JgK\n",
      "\n",
      "‚ÄúEla nos contou como a m√£e da Beyonc√© (Tina Knowles) fez pra ela um vestido que apenas ela e a Beyonc√© tem.‚Äù ‚Äî Anthony Ramos, ator do filme, ‚ÄòA Star Is Born‚Äô. https://t.co/jorrqGEnDR\n",
      "\n",
      "Bruno Mars joins Lionel Richie, Whitney Houston, Mariah Carey, Beyonc√© and Lady Gaga as the only artists with 3+ Hot 100 top 10s from each of their first 3 albums.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.all_text, end='\\n\\n') for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-16 21:58:55\n",
      "2018-01-16 21:58:53\n",
      "2018-01-16 21:58:49\n",
      "2018-01-16 21:58:46\n",
      "2018-01-16 21:58:46\n",
      "2018-01-16 21:58:45\n",
      "2018-01-16 21:58:45\n",
      "2018-01-16 21:58:43\n",
      "2018-01-16 21:58:42\n",
      "2018-01-16 21:58:41\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.created_at_datetime) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for Android\n",
      "Twitter for iPhone\n",
      "Twitter for iPhone\n",
      "Twitter for Android\n",
      "Twitter for iPhone\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.generator.get(\"name\")) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, we have some tweets. For interactive environments and other cases where you don't care about collecting your data in a single load or don't need to operate on the stream of tweets or counts directly, I recommend using this convenience function.\n",
    "\n",
    "\n",
    "## Working with the ResultStream\n",
    "\n",
    "The ResultStream object will be powered by the `search_args`, and takes the rules and other configuration parameters, including a hard stop on number of pages to limit your API call usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultStream: \n",
      "\t{\n",
      "    \"username\":null,\n",
      "    \"endpoint\":\"https:\\/\\/api.twitter.com\\/1.1\\/tweets\\/search\\/30day\\/dev.json\",\n",
      "    \"rule_payload\":{\n",
      "        \"query\":\"beyonce\",\n",
      "        \"maxResults\":100\n",
      "    },\n",
      "    \"tweetify\":true,\n",
      "    \"max_results\":500\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=500,\n",
    "                  max_pages=1,\n",
    "                  **premium_search_args)\n",
    "\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function, `.stream`, that seamlessly handles requests and pagination for a given query. It returns a generator, and to grab our 500 tweets that mention `beyonce` we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = list(rs.stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are lazily parsed using our Tweet Parser, so tweet data is very easily extractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfect predicted to drop off #1 a week after they cut the beyonc√© credit https://t.co/tKZ2flay8A\n",
      "¬´¬†quand Beyonce a dit ¬´¬†c‚Äôest pas la tromperie qui fait mal, c‚Äôest le genre de personne avec qui il me trompe, que mnt je dois le regarder dans les yeux et savoir qu‚Äôelle avait la moiti√© de ce que j‚Äôai alors qu‚Äôelle n‚Äôest m√™me pas la moiti√© de ce que je suis¬†¬ª j‚Äôai compris\n",
      "\"Todo esse √≥dio que a Beyonc√© recebe √© culpa da Beyhive\"\n",
      "\n",
      "Uma f√£-base que ama a maior artista da atualidade,ativista, empoderada na maioria das vezes inteligente e com argumentos com fundamento te assusta? \n",
      "Vadias b√°sicas, curvem-se para a Beyhive! üçØüêù https://t.co/TCBBb1lmMk\n",
      "Imagine Nicki Minaj, Beyonc√©, Rihanna and Ariana Grande doing a song together. A lady marmalade remake with them four would be Iconic. https://t.co/PqoKEtmMgS\n",
      "NEW EMINEM INTERVIEW IS HERE. \n",
      "\n",
      "\"I don't know what I'm gonna do when I can't rap anymore. I probably fucking... https://t.co/ficwtt0lGc\n",
      "I‚Äôm ready for more Beyonc√© music but am I ready for more Beyonc√© discourse https://t.co/srWdBMAWDb\n",
      "PERFECT DUET\n",
      "Ed Sheeran with Beyonc√©\n",
      "I found a love for me... https://t.co/hog1NNTpUM\n",
      "si quelqu‚Äôun vous traite de moche dites vous qu‚Äôil existe des gens qui trouvent rihanna et beyonce moche donc au final √™tre moche √ßa signifie quoi ???\n",
      "@SamuelMpiri @EmmanuelMacron @rihanna @Sarah_Bnz sarah&gt;beyonc√©\n",
      "o pai da beyonc√© obrigava ela a cantar enquanto corria numa esteira para que ela aprendesse a controlar a respira√ß√£o e assim dan√ßar enquanto canta sem perder f√¥lego ou desafinar https://t.co/fQdANVPs14\n"
     ]
    }
   ],
   "source": [
    "# using unidecode to prevent emoji/accents printing \n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts Endpoint\n",
    "\n",
    "We can also use the Search API Counts endpoint to get counts of tweets that match our rule. Each request will return up to *30* results, and each count request can be done on a minutely, hourly, or daily basis. The underlying `ResultStream` object will handle converting your endpoint to the count endpoint, and you have to specify the `count_bucket` argument when making a rule to use it.\n",
    "\n",
    "The process is very similar to grabbing tweets, but has some minor differences.\n",
    "\n",
    "\n",
    "_Caveat - premium sandbox environments do NOT have access to the Search API counts endpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_rule = gen_rule_payload(\"beyonce\", count_bucket=\"day\")\n",
    "\n",
    "counts = collect_results(count_rule, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are pretty straightforward and can be rapidly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': 40182, 'timePeriod': '201801160000'},\n",
       " {'count': 61955, 'timePeriod': '201801150000'},\n",
       " {'count': 59688, 'timePeriod': '201801140000'},\n",
       " {'count': 44023, 'timePeriod': '201801130000'},\n",
       " {'count': 46618, 'timePeriod': '201801120000'},\n",
       " {'count': 41527, 'timePeriod': '201801110000'},\n",
       " {'count': 47060, 'timePeriod': '201801100000'},\n",
       " {'count': 65513, 'timePeriod': '201801090000'},\n",
       " {'count': 95272, 'timePeriod': '201801080000'},\n",
       " {'count': 162926, 'timePeriod': '201801070000'},\n",
       " {'count': 106393, 'timePeriod': '201801060000'},\n",
       " {'count': 93565, 'timePeriod': '201801050000'},\n",
       " {'count': 110436, 'timePeriod': '201801040000'},\n",
       " {'count': 127564, 'timePeriod': '201801030000'},\n",
       " {'count': 131984, 'timePeriod': '201801020000'},\n",
       " {'count': 176206, 'timePeriod': '201801010000'},\n",
       " {'count': 57241, 'timePeriod': '201712310000'},\n",
       " {'count': 72280, 'timePeriod': '201712300000'},\n",
       " {'count': 72083, 'timePeriod': '201712290000'},\n",
       " {'count': 76396, 'timePeriod': '201712280000'},\n",
       " {'count': 61610, 'timePeriod': '201712270000'},\n",
       " {'count': 55122, 'timePeriod': '201712260000'},\n",
       " {'count': 59121, 'timePeriod': '201712250000'},\n",
       " {'count': 106245, 'timePeriod': '201712240000'},\n",
       " {'count': 114782, 'timePeriod': '201712230000'},\n",
       " {'count': 73349, 'timePeriod': '201712220000'},\n",
       " {'count': 89186, 'timePeriod': '201712210000'},\n",
       " {'count': 192418, 'timePeriod': '201712200000'},\n",
       " {'count': 85573, 'timePeriod': '201712190000'},\n",
       " {'count': 57850, 'timePeriod': '201712180000'},\n",
       " {'count': 70483, 'timePeriod': '201712170000'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dated searches / Full Archive Search\n",
    "\n",
    "\n",
    "Let's make a new rule and pass it dates this time.\n",
    "\n",
    "`gen_rule_payload` takes dates of the forms `YYYY-mm-DD` and `YYYYmmDD`.\n",
    "\n",
    "\n",
    "**Note that this will only work with the full archive search option**, which is available to my account only via the enterprise options. Full archive search will likely require a different endpoint or access method; please see your developer console for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"from:jack\",\"maxResults\":500,\"toDate\":\"201710300000\",\"fromDate\":\"201709010000\"}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"from:jack\",\n",
    "                        from_date=\"2017-09-01\",\n",
    "                        to_date=\"2017-10-30\",\n",
    "                        results_per_call=500)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More clarity on our private information policy and enforcement. Working to build as much direct context into the product too https://t.co/IrwBexPrBA\n",
      "To provide more clarity on our private information policy, we‚Äôve added specific examples of what is/is not a violation and insight into what we need to remove this type of content from the service. https://t.co/NGx5hh2tTQ\n",
      "Launching violent groups and hateful images/symbols policy on November 22nd https://t.co/NaWuBPxyO5\n",
      "We will now launch our policies on violent groups and hateful imagery and hate symbols on Nov 22. During the development process, we received valuable feedback that we‚Äôre implementing before these are published and enforced. See more on our policy development process here üëá https://t.co/wx3EeH39BI\n",
      "@WillStick @lizkelley Happy birthday Liz!\n",
      "Off-boarding advertising from all accounts owned by Russia Today (RT) and Sputnik.\n",
      "\n",
      "We‚Äôre donating all projected earnings ($1.9mm) to support external research into the use of Twitter in elections, including use of malicious automation and misinformation. https://t.co/zIxfqqXCZr\n",
      "@TMFJMo @anthonynoto Thank you\n",
      "@gasca @stratechery @Lefsetz letter\n",
      "@gasca @stratechery Bridgewater‚Äôs Daily Observations\n",
      "Yup!!!! ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #davechappelle https://t.co/ybSGNrQpYF\n",
      "@ndimichino Sometimes\n",
      "Setting up at @CampFlogGnaw https://t.co/nVq8QjkKsf\n"
     ]
    }
   ],
   "source": [
    "# usiing unidecode only to \n",
    "[print(tweet.all_text) for tweet in tweets[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"from:jack\",\"toDate\":\"201710300000\",\"fromDate\":\"201709200000\",\"bucket\":\"day\"}\n"
     ]
    }
   ],
   "source": [
    "rule = gen_rule_payload(\"from:jack\",\n",
    "                        from_date=\"2017-09-20\",\n",
    "                        to_date=\"2017-10-30\",\n",
    "                        count_bucket=\"day\",\n",
    "                        results_per_call=500)\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = collect_results(rule, max_results=500, result_stream_args=enterprise_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timePeriod': '201710290000', 'count': 0}\n",
      "{'timePeriod': '201710280000', 'count': 0}\n",
      "{'timePeriod': '201710270000', 'count': 3}\n",
      "{'timePeriod': '201710260000', 'count': 6}\n",
      "{'timePeriod': '201710250000', 'count': 4}\n",
      "{'timePeriod': '201710240000', 'count': 4}\n",
      "{'timePeriod': '201710230000', 'count': 0}\n",
      "{'timePeriod': '201710220000', 'count': 0}\n",
      "{'timePeriod': '201710210000', 'count': 3}\n",
      "{'timePeriod': '201710200000', 'count': 2}\n",
      "{'timePeriod': '201710190000', 'count': 1}\n",
      "{'timePeriod': '201710180000', 'count': 6}\n",
      "{'timePeriod': '201710170000', 'count': 2}\n",
      "{'timePeriod': '201710160000', 'count': 2}\n",
      "{'timePeriod': '201710150000', 'count': 1}\n",
      "{'timePeriod': '201710140000', 'count': 64}\n",
      "{'timePeriod': '201710130000', 'count': 3}\n",
      "{'timePeriod': '201710120000', 'count': 4}\n",
      "{'timePeriod': '201710110000', 'count': 8}\n",
      "{'timePeriod': '201710100000', 'count': 4}\n",
      "{'timePeriod': '201710090000', 'count': 1}\n",
      "{'timePeriod': '201710080000', 'count': 0}\n",
      "{'timePeriod': '201710070000', 'count': 0}\n",
      "{'timePeriod': '201710060000', 'count': 1}\n",
      "{'timePeriod': '201710050000', 'count': 3}\n",
      "{'timePeriod': '201710040000', 'count': 5}\n",
      "{'timePeriod': '201710030000', 'count': 8}\n",
      "{'timePeriod': '201710020000', 'count': 5}\n",
      "{'timePeriod': '201710010000', 'count': 0}\n",
      "{'timePeriod': '201709300000', 'count': 0}\n",
      "{'timePeriod': '201709290000', 'count': 0}\n",
      "{'timePeriod': '201709280000', 'count': 9}\n",
      "{'timePeriod': '201709270000', 'count': 41}\n",
      "{'timePeriod': '201709260000', 'count': 13}\n",
      "{'timePeriod': '201709250000', 'count': 6}\n",
      "{'timePeriod': '201709240000', 'count': 7}\n",
      "{'timePeriod': '201709230000', 'count': 3}\n",
      "{'timePeriod': '201709220000', 'count': 0}\n",
      "{'timePeriod': '201709210000', 'count': 1}\n",
      "{'timePeriod': '201709200000', 'count': 7}\n"
     ]
    }
   ],
   "source": [
    "[print(c) for c in counts];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
