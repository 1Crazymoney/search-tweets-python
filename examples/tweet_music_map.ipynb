{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Twitter Search API\n",
    "\n",
    "The goal of this notebook is to provide a strong demo of using the new python twitter search API wrapper to Do Data Scienceâ„¢.\n",
    "\n",
    "\n",
    "Twitter data has a massive potential across many domains. What if you were curious about music patterns across the world? \n",
    "\n",
    "\n",
    "\n",
    "Let's get our notebook started with some imports and basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tweet_parser.tweet import Tweet\n",
    "from tweet_parser.getter_methods.tweet_geo import get_profile_location\n",
    "\n",
    "from twittersearch.result_stream import ResultStream\n",
    "from twittersearch.utils import *\n",
    "\n",
    "# the following makes working in a notebook a bit easier, as you don't have to have new cells for all output\n",
    "from IPython.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the API interactively\n",
    "\n",
    "We'll define some high-level information here. For convenience, I've stored my password in an environemnt variable and we'll be using both the counts and search endpoints. I will generate the relevant arguments that we'll use for the rest of our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TWITTER_SEARCH_ACCOUNT_NAME\"] = \"\"\n",
    "os.environ[\"TWITTER_SEARCH_PW\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"agonzales@twitter.com\"\n",
    "search_api = \"fullarchive\"\n",
    "endpoint_label = \"ogformat.json\"\n",
    "\n",
    "search_endpoint = gen_endpoint(search_api,\n",
    "                               os.environ[\"TWITTER_SEARCH_ACCOUNT_NAME\"],\n",
    "                               endpoint_label,\n",
    "                               count_endpoint=False)\n",
    "\n",
    "count_endpoint = gen_endpoint(search_api,\n",
    "                              os.environ[\"TWITTER_SEARCH_ACCOUNT_NAME\"],\n",
    "                              endpoint_label,\n",
    "                              count_endpoint=True)\n",
    "\n",
    "search_args = {\"username\": username,\n",
    "               \"password\": os.environ[\"TWITTER_SEARCH_PW\"],\n",
    "               \"url\": search_endpoint }\n",
    "count_args = {\"username\": username,\n",
    "               \"password\": os.environ[\"TWITTER_SEARCH_PW\"],\n",
    "              \"url\": count_endpoint }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The power of the search api comes from the rich query language that it exposes to devleopers. Instead of manually filtering tweets from the free 1% streaming api, we can rapidly explore *all* tweets, all the way back to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">just setting up my twttr</p>&mdash; jack (@jack) <a href=\"https://twitter.com/jack/status/20\">March 21, 2006</a></blockquote> <script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search API also has a `counts` endpoint, which instead of returning tweet data, returns the number of tweets that match your search critera that can be divied up into convenient buckets. quick explorations with the `count` endpoint can save you time and API calls.\n",
    "\n",
    "\n",
    "Let's explore some tweets that mention musicians and how to make powertrack rules.\n",
    "\n",
    "When using the helper method `gen_rule_payload`, you must define your powertrack rule. We'll start by matching tweets that mention \"taylor swift\" and use the counts api to refine our searches from there.\n",
    "\n",
    "As a side note, in a notebook environemnt, i will break the DRY principle a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "\"taylor swift\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "count_rule = gen_rule_payload(_rule,\n",
    "                        from_date=\"2016-09-01\",\n",
    "                        to_date=\"2017-09-01\",\n",
    "                        max_results=500, \n",
    "                        count_bucket=\"day\")\n",
    "\n",
    "count_rule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main point of entry to the API is the `ResultStream` object and it's primary function, `.stream()`. We'll pass it our connection and authentication arguments, the jsonified rule payload, and either the `max_tweets` argument or the `max_pages` argument. By default, the `ResultStream` will paginate results from the API for you, but this parameter can effectively limit the number of calls used.\n",
    "\n",
    "When using the Counts API, a single call returns a json array of count records, one per count bucket. For our year-long range, a single call of 'day' buckets will get all the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_stream = ResultStream(**count_args,\n",
    "                           rule_payload=count_rule,\n",
    "                           max_tweets=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ResultStream` object can be inspected with your various args. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(count_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object has one main entry point, which returns a generator of results from the API. In many cases, you'll want to fetch all the results from this stream of data, which can be done as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_results_gen = count_stream.stream()\n",
    "\n",
    "count_results_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = list(count_results_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described earlier, we get back a quick count of tweets matching our rule in each bucket. We can quickly visualize this with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .sum()\n",
    " \n",
    ")\n",
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .plot(title=_rule)\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so there are 12 million matching tweets in our yearly history, with big spikes around the time Taylor does something newsworthy. We probably missed something here, which is that the exact match of \"taylor swift\" will miss all the mentions with her in them. Let's redo our search with a new rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "\"taylor swift\" OR (has:mentions @taylorswift13)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "count_rule = gen_rule_payload(_rule,\n",
    "                        from_date=\"2016-09-01\",\n",
    "                        to_date=\"2017-09-01\",\n",
    "                        max_results=500, \n",
    "                        count_bucket=\"day\")\n",
    "\n",
    "count_rule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can shorten our previous interaction with the `ResultStream` object to something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = list(ResultStream(**count_args,\n",
    "                           rule_payload=count_rule,\n",
    "                           max_tweets=500)\n",
    "              .stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .sum()\n",
    " \n",
    ")\n",
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .plot(title=_rule)\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we get similar patterns of tweets, but have an additional 5 million with which we can work, and we haven't began including other things people might mention about her, such as her album or song names, or phrases like \"I'm listening to tswift's new album\". Let's add in her common nickname to the counts on more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "\"taylor swift\"\n",
    "OR (has:mentions @taylorswift13)\n",
    "OR \"tswift\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "count_rule = gen_rule_payload(_rule,\n",
    "                        from_date=\"2016-09-01\",\n",
    "                        to_date=\"2017-09-01\",\n",
    "                        max_results=500, \n",
    "                        count_bucket=\"day\")\n",
    "\n",
    "count_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = list(ResultStream(**count_args,\n",
    "                           rule_payload=count_rule,\n",
    "                           max_tweets=500)\n",
    "              .stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .sum()\n",
    " \n",
    ")\n",
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .plot(title=_rule)\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardly any difference, so let's move on. \n",
    "\n",
    "Powertrack rules also allow you to find tweets that have explicit information, such as geographical location data. We'll want to examine regional differences in music patterns, so let's see how many of our tweets have that data using the `has:geo` operator. \n",
    "\n",
    "Note that geo tags are turned OFF by default, so there are usually FAR fewer tweets that have geo information than those without. We offer a `profile_geo` operator that will expose the home geo of the user, which can be a very reasonable proxy for tweet geo in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "(\"taylor swift\" OR (has:mentions @taylorswift13) OR \"tswift\")\n",
    "has:geo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "count_rule = gen_rule_payload(_rule,\n",
    "                        from_date=\"2016-09-01\",\n",
    "                        to_date=\"2017-09-01\",\n",
    "                        max_results=500, \n",
    "                        count_bucket=\"day\")\n",
    "\n",
    "count_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = list(ResultStream(**count_args,\n",
    "                           rule_payload=count_rule,\n",
    "                           max_tweets=500)\n",
    "              .stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .sum()\n",
    " \n",
    ")\n",
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .plot(title=_rule)\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we have wildly fewer tweets that match our criteria, but let's see what we can do with the `profile_geo` enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "(\"taylor swift\" OR (has:mentions @taylorswift13) OR \"tswift\")\n",
    "(has:geo\n",
    "OR\n",
    "has:profile_geo)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "count_rule = gen_rule_payload(_rule,\n",
    "                        from_date=\"2016-09-01\",\n",
    "                        to_date=\"2017-09-01\",\n",
    "                        max_results=500, \n",
    "                        count_bucket=\"day\")\n",
    "\n",
    "count_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = list(ResultStream(**count_args,\n",
    "                           rule_payload=count_rule,\n",
    "                           max_tweets=500)\n",
    "              .stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .sum()\n",
    " \n",
    ")\n",
    "(pd.DataFrame(counts)\n",
    " .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    " .set_index(\"timePeriod\")\n",
    " .sort_index()\n",
    " .plot(title=_rule)\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have a reasonable idea of what we're working with here and we haven't pulled a single tweet yet. \n",
    "\n",
    "Let's take a look at some tweet data with our finer-grained geo search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rule = \"\"\"\n",
    "(\"taylor swift\" OR (has:mentions @taylorswift13) OR \"tswift\")\n",
    "has:geo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "search_rule = gen_rule_payload(_rule,\n",
    "                               from_date=\"2016-09-01\",\n",
    "                               to_date=\"2017-09-01\",\n",
    "                               max_results=500\n",
    "                              )\n",
    "\n",
    "search_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = list(ResultStream(**search_args, rule_payload=search_rule, max_tweets=500)\n",
    "              .stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the python interface to the search API, tweets are automatically parsed via the [Tweet_parser library](https://github.com/tw-ddis/tweet_parser), which makes working with them fairly straightforward. We've abstracted out many of the annoying details of working with raw tweet data and provided a straightforward API with access to lots of tweet data. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = tweets[200]\n",
    "t.created_at_datetime\n",
    "t.screen_name\n",
    "t.bio\n",
    "t.text\n",
    "\n",
    "\n",
    "t[\"place\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this person doesn't seem to care much for Taylor's new music. (Shrug).\n",
    "\n",
    "\n",
    "Before we move on, we'll pause to define a few functions that will help with grabbing the geojson data from a tweet. Recall that the `has:geo` operator will return tweets that are tagged at a Twitter-defined `Place` (in this case, the city of Coralville, IA). Places are given a bounding box, which may not be useful for your application (i have a map in mind...).\n",
    "\n",
    "We cannot cover all use cases for the tweet parser, but it's easily extentible, or you can use methods that work on dicts because each tweet is really a subclass of `dict`. \n",
    "\n",
    "To demonstrate the need for this here, let's grab tweets that have *exact* coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.geo_coordinates for t in tweets if t.geo_coordinates] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are going to generate a map or work with single coordinates, we'll have to turn boudning boxes into sigular points and have a single function that extracts the geo data from each tweet.\n",
    "\n",
    "\n",
    "Note - the coordinates are returned as [LONG, LAT], and in the parsing functions, I will flip those to the required downstream format [LAT, LONG]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from tweet_parser.tweet_checking import is_original_format\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    mean_bbox = lambda x: list(np.array(x).mean(axis=0))\n",
    "except ImportError:\n",
    "    mean_bbox = lambda x: (reduce(lambda y, z: y + z, x) / len(x))\n",
    "\n",
    "def get_profile_geo_coords(tweet):\n",
    "    geo = tweet.profile_location.get(\"geo\")\n",
    "    coords = geo.get(\"coordinates\") # in [LONG, LAT]\n",
    "    if coords:\n",
    "        long, lat = coords\n",
    "    return lat, long\n",
    "\n",
    "\n",
    "def get_place_coords(tweet, est_center=False):\n",
    "    \"\"\"\n",
    "    Places are formal spots that define a bounding box around a place.\n",
    "    Each coordinate pair in the bounding box is a set of [[lat, long], [lat, long]]\n",
    "    pairs.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_bbox_ogformat():\n",
    "        _place = tweet.get(\"place\")\n",
    "        if _place is None:\n",
    "            return None\n",
    "    \n",
    "        return (_place\n",
    "                .get(\"bounding_box\")\n",
    "                .get(\"coordinates\")[0])\n",
    "\n",
    "    def get_bbox_asformat():\n",
    "        _place = tweet.get(\"location\")\n",
    "        if _place is None:\n",
    "            return None\n",
    "        return (_place\n",
    "                .get(\"geo\")\n",
    "                .get(\"coordinates\")[0])\n",
    "        \n",
    "    bbox = get_bbox_ogformat() if is_original_format(tweet) else get_bbox_asformat()\n",
    "\n",
    "    return mean_bbox(bbox) if est_center else bbox\n",
    "\n",
    "\n",
    "def get_exact_geo_coords(tweet):\n",
    "    geo = tweet.get(\"geo\")\n",
    "    if geo is None:\n",
    "        return None\n",
    "    \n",
    "    # coordinates.coordinates is [LONG, LAT]\n",
    "    # geo.coordinates is [LAT, LONG]\n",
    "    field = \"geo\" if is_original_format(tweet) else \"geo\"\n",
    "    coords = tweet.get(\"geo\").get(\"coordinates\")\n",
    "    return coords\n",
    "\n",
    "\n",
    "def get_a_geo_coordinate(tweet):\n",
    "    geo = get_exact_geo_coords(tweet)\n",
    "    lat, long = geo if geo else (None, None)\n",
    "    if lat:\n",
    "        return lat, long\n",
    "    long, lat = get_place_coords(tweet, est_center=True)\n",
    "    return lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_a_geo_coordinate(t)\n",
    "t[\"place\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our highest function will grab the center of a bounding box. This works really well when we have a city-or-smaller place and is not as useful for a profile geo of \"Texas\", but so it goes.\n",
    "\n",
    "\n",
    "## Working with the tweet stream / Let's build a map\n",
    "\n",
    "\n",
    "Given the streaming format of tweets that is returned by our api, we can pull large amounts of data interactively and build functions that help organize it to our needs directly. \n",
    "\n",
    "As this example is going to eventually go beyond Taylor Swift, we'll define a wrapper function that takes an input ResultStream object and returns a pandas dataframe with our geo coordinate information and other basic information from tweets. it will correctly filter and index the dataframe, giving us something back that we can rapidly use to Make Maps. We'll use another utility function that converts the [lat, long] pairs to web mercator format ([meters_x, meters_y]) format that is required by some plotting libraries. I lightly adapted this from an example in Datashader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latlng_to_meters(df, lat_name, lng_name):\n",
    "    \"\"\"\n",
    "    Taken and modified from the datashader notebooks \n",
    "    \"\"\"\n",
    "    lat = df[lat_name]\n",
    "    lng = df[lng_name]\n",
    "    origin_shift = 2 * np.pi * 6378137 / 2.0\n",
    "    mx = lng * origin_shift / 180.0\n",
    "    my = np.log(np.tan((90 + lat) * np.pi / 360.0)) / (np.pi / 180.0)\n",
    "    my = my * origin_shift / 180.0\n",
    "    return df.assign(mx=mx).assign(my=my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_geo_collector(result_stream, tag, fields=None):\n",
    "    if fields is None:\n",
    "        fields = [\"id\", \"created_at_datetime\", \"text\"]\n",
    "    \n",
    "    coords = []\n",
    "    print(\"collecting tweets for {}\".format(tag))\n",
    "    for tweet in result_stream.stream():\n",
    "        attrs = (tweet.__getattribute__(field)\n",
    "                   for field in fields)\n",
    "        try:\n",
    "            _coords = get_a_geo_coordinate(tweet)\n",
    "            coords.append(list(it.chain.from_iterable([attrs, _coords])))\n",
    "        except AttributeError:\n",
    "            print(\"error in geo\")\n",
    "            print(tweet.id, tweet.text)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "    columns = list(it.chain.from_iterable([fields, [\"lat\", \"long\"]]))\n",
    "    \n",
    "    df = (pd.DataFrame(coords, columns=columns)\n",
    "          .pipe(latlng_to_meters, \"lat\", \"long\")\n",
    "          .drop([\"lat\", \"long\"], axis=1)\n",
    "          .assign(tag=tag)\n",
    "         )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ResultStream(**search_args, rule_payload=search_rule, max_tweets=2000)\n",
    "# rs.artist = \"taylor_swift\"\n",
    "df = tweet_geo_collector(rs, tag=\"taylor_swift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .set_index(\"created_at_datetime\")\n",
    " .sort_index()\n",
    " .resample(\"D\")\n",
    " .size()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Bokeh to do some plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "from bokeh.models import WMTSTileSource\n",
    "from bokeh.tile_providers import STAMEN_TONER\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import ColumnDataSource, figure\n",
    "from bokeh.models import HoverTool, value\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "tiles = {'OpenMap': WMTSTileSource(url='http://c.tile.openstreetmap.org/{Z}/{X}/{Y}.png'),\n",
    "         'ESRI': WMTSTileSource(url='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{Z}/{Y}/{X}.jpg'),\n",
    "         'Wikipedia': WMTSTileSource(url='https://maps.wikimedia.org/osm-intl/{Z}/{X}/{Y}@2x.png'),\n",
    "         'Stamen': WMTSTileSource(url=\"http://tile.stamen.com/toner-background/{z}/{x}/{y}.png\", )\n",
    "         }\n",
    "\n",
    "\n",
    "def plot_tweets(df, x_col=\"mx\", y_col=\"my\", tile=\"Stamen\", title=\"title\"):\n",
    "    # add our DataFrame as a ColumnDataSource for Bokeh\n",
    "    plot_data = ColumnDataSource(df)\n",
    "    # create the plot and configure the\n",
    "    # title, dimensions, and tools\n",
    "    plot = figure(title=title,\n",
    "                  plot_width=800,\n",
    "                  plot_height=800,\n",
    "                  tools= ('pan, wheel_zoom, box_zoom, reset'),\n",
    "                  active_scroll='wheel_zoom')\n",
    "\n",
    "    # add a hover tool to display words on roll-over\n",
    "    plot.add_tools(HoverTool(tooltips = '@text'))\n",
    "\n",
    "    # draw the words as circles on the plot\n",
    "    plot.circle(x=x_col, y=y_col, source=plot_data,\n",
    "                     color=u'blue', line_alpha=0.1, fill_alpha=0.1,\n",
    "                     size=3, hover_line_color='black')\n",
    "\n",
    "    # configure visual elements of the plot\n",
    "    plot.title.text_font_size = value('12pt')\n",
    "    plot.xaxis.visible = False\n",
    "    plot.yaxis.visible = False\n",
    "    plot.grid.grid_line_color = None\n",
    "    plot.outline_line_color = None\n",
    "    plot.add_tile(tiles[tile], alpha=0.25)\n",
    "    return plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plot_tweets(df, title=json.loads(search_rule)[\"query\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, wrapping up the first section, we have an interactive map of where tweets that mention taylor-swift related terms in very little code. \n",
    "\n",
    "\n",
    "We'll scale up in the next section by adding both \n",
    "- more tweets\n",
    "- more artists\n",
    "\n",
    "## Dynamically generating rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_rule = \"\"\"\n",
    "((\"{exact_name}\") OR (has:mentions @{handle}))\n",
    "has:geo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "gen_search_payload_ = partial(gen_rule_payload,\n",
    "                            max_results=500,\n",
    "                            from_date=\"2016-09-01\",\n",
    "                            to_date=\"2017-09-01\",\n",
    "                            )\n",
    "\n",
    "gen_count_payload_ = partial(gen_rule_payload,\n",
    "                            max_results=500,\n",
    "                            from_date=\"2016-09-01\",\n",
    "                            to_date=\"2017-09-01\",\n",
    "                            count_bucket='day'\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "artists_names = [\"taylor swift\",\n",
    "                 \"katy perry\",\n",
    "                 \"beyonce\",\n",
    "                 \"lady gaga\",\n",
    "                 \"britney spears\"\n",
    "                ]\n",
    "\n",
    "artists = {\"taylor swift\": \"taylorswift13\",\n",
    "           \"katy perry\": \"katyperry\",\n",
    "           \"beyonce\": \"beyonce\",\n",
    "           \"lady gaga\": \"ladygaga\",\n",
    "           \"britney spears\": \"britneyspears\"\n",
    "           \n",
    "          }\n",
    "\n",
    "\n",
    "artist_count_rules = [gen_count_payload_(base_rule.format(exact_name=name, handle=handle))\n",
    "                for name, handle in artists.items()]\n",
    "artist_search_rules = [gen_search_payload_(base_rule.format(exact_name=name, handle=handle))\n",
    "                for name, handle in artists.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams = [ResultStream(**count_args,\n",
    "                        rule_payload=rule,\n",
    "                        max_tweets=2000)\n",
    "           for rule in artist_rules]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (pd.concat([pd.DataFrame(list(rs.stream())).assign(artist=artist)\n",
    "                    for rs, artist in zip(streams, artists_names)])\n",
    "          .assign(timePeriod=lambda df: pd.to_datetime(df[\"timePeriod\"]))\n",
    "          .set_index([\"timePeriod\", \"artist\"])\n",
    "          .sort_index()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.unstack().sum()\n",
    "counts.unstack()['count'].plot()\n",
    "counts.unstack()['count'].rolling(\"14D\").mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [ResultStream(**search_args,\n",
    "                        rule_payload=rule,\n",
    "                        max_tweets=2000)\n",
    "           for rule in artist_search_rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [tweet_geo_collector(stream, tag)\n",
    "           for stream, tag in zip(streams, artists.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .set_index(\"created_at_datetime\")\n",
    " .sort_index()\n",
    " .groupby([pd.TimeGrouper(\"D\"), \"tag\"])\n",
    " .size()\n",
    " .to_frame(\"tweets\")\n",
    " [\"tweets\"]\n",
    " .unstack()\n",
    " .fillna(0)\n",
    " .plot()\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def even_sample(df, cat_col):\n",
    "    cats = df[cat_col].unique()\n",
    "    vc = df[cat_col].value_counts()\n",
    "    min_count = vc.min()\n",
    "    res = []\n",
    "    for cat in cats:\n",
    "        res.append(df[df[cat_col] == cat].sample(min_count))\n",
    "    return pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "even_df = even_sample(df, \"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models.widgets import Panel, Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plot_tweets(df.query(\"tag == 'lady gaga'\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_chainz = plot_tweets(df.query(\"tag == 'rhi'\"))\n",
    "ku = plot_tweets(df.query(\"tag == 'keith urban'\"))\n",
    "ts = plot_tweets(df.query(\"tag == 'taylor swift'\"))\n",
    "bey = plot_tweets(df.query(\"tag == 'beyonce'\"))\n",
    "\n",
    "tabs = Tabs(tabs=[Panel(child=two_chainz, title=\"2 chainz\"),\n",
    "                 # Panel(child=ku, title=\"keith urban\"),\n",
    "                  Panel(child=ts, title=\"taylor swift\"),\n",
    "                 # Panel(child=bey, title=\"beyonce\")\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, reset_output, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_output()\n",
    "# output_file(\"bokeh_tabs.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file(\"test_bokeh.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(tabs, filename=\"test_bokeh.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh import palettes\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "from datashader.bokeh_ext import InteractiveImage\n",
    "\n",
    "from cartopy import crs\n",
    "\n",
    "\n",
    "import geoviews as gv\n",
    "\n",
    "import holoviews as hv\n",
    "\n",
    "from holoviews.operation.datashader import aggregate, shade, datashade, dynspread\n",
    "\n",
    "\n",
    "hv.notebook_extension('mpl', 'bokeh')\n",
    "\n",
    "\n",
    "def gen_col_points(categories, colormap):\n",
    "    inv_cats = {k: k for k in categories}\n",
    "    color_points = hv.NdOverlay({inv_cats[k]: gv.Points([0,0],\n",
    "                                                        crs=crs.PlateCarree(),\n",
    "                                                        label=inv_cats[k])\n",
    "                                 (style=dict(color=v))\n",
    "                                 for k, v in colormap.items()})\n",
    "    return color_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_df = df.assign(tag=lambda df: df[\"tag\"].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_min, y_min, x_max, y_max = (plot_df.mx.values.min(),\n",
    "                              plot_df.my.values.min(),\n",
    "                              plot_df.mx.values.max(),\n",
    "                              plot_df.my.values.max())\n",
    "x_range=(x_min, x_max)\n",
    "y_range=(y_min, y_max)\n",
    "color_key = dict(zip(artists, palettes.Category10[len(artists)]))\n",
    "shade_defaults = dict(x_range=x_range,\n",
    "                      y_range=y_range,\n",
    "                      width=1200,\n",
    "                      height=660)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%output filename=\"artist_datashaded_points\"\n",
    "%%opts Overlay [width=800 height=600 xaxis=None yaxis=None show_grid=False ] (background_alpha=0.1) \n",
    "%%opts Shape (fill_color=None line_width=1.5) [apply_ranges=False] \n",
    "%%opts Points [apply_ranges=False tools=[]]\n",
    "%%opts WMTS (alpha=0.25)\n",
    "\n",
    "# shade_defaults = dict(x_range=(x_max, x_min),\n",
    "                      # y_range=(y_max, y_min),\n",
    "                      # width=1200,\n",
    "                      # height=660)\n",
    "\n",
    "shaded_points = datashade(hv.Points(gv.Dataset(plot_df,\n",
    "                                               kdims=[\"mx\", \"my\"],\n",
    "                                               vdims=[\"tag\"])),\n",
    "                          cmap=color_key,\n",
    "                          element_type=gv.Image,\n",
    "                          aggregator=ds.count_cat(\"tag\"),\n",
    "                          **shade_defaults, \n",
    "                         )\n",
    "\n",
    "color_points = gen_col_points(color_key.keys(), color_key)\n",
    "\n",
    "map_ = gv.WMTS(tiles[\"Stamen\"]) * dynspread(shaded_points,\n",
    "                                            max_px=1,\n",
    "                                            threshold=0.5) * color_points\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_holo)",
   "language": "python",
   "name": "conda_holo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
